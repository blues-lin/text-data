{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: 30\n",
      "Convert to docs: 239\n",
      "Array shape: x:(4331, 100), y:(7,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# Prepare training data.\n",
    "from lib import vectorize\n",
    "from lib import text_searcher\n",
    "\n",
    "vec = vectorize.Vectorizer(\"char1.txt\", \"label.txt\")\n",
    "searcher = text_searcher.TextSearcher(\"corpus.sqlite\")\n",
    "TEXT_LENGTH = 100\n",
    "\n",
    "trainingFile = open(\"training_terms.tsv\", \"r\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "training_terms = []\n",
    "for row in trainingFile:\n",
    "    r = row.split(\"\\t\")\n",
    "    x = r[0]\n",
    "    y = r[1].split(\" \")\n",
    "    training_terms.append((x, y))\n",
    "\n",
    "\n",
    "training_data = []\n",
    "for terms in training_terms:\n",
    "    query = terms[0]\n",
    "    n = 0\n",
    "    searcher = text_searcher.TextSearcher(\"corpus.sqlite\")\n",
    "    for doc in searcher.genDocs(query):\n",
    "        n += 1\n",
    "        if n >= 10:\n",
    "            break\n",
    "        training_data.append((doc, terms[1]))\n",
    "\n",
    "\n",
    "arrayList = []\n",
    "for data in training_data:\n",
    "    x = vec.vectorize(data[0], TEXT_LENGTH)\n",
    "    y = vec.vectorizeLabel(data[1])\n",
    "    arrayList.append((x, y))\n",
    "\n",
    "\n",
    "print(\"Loaded training data: {}\".format(len(training_terms)))\n",
    "print(\"Convert to docs: {}\".format(len(training_data)))\n",
    "print(\"Array shape: x:{}, y:{}\".format(arrayList[0][0].shape, arrayList[0][1].shape))\n",
    "\n",
    "\n",
    "# Model hyperparameter setting.\n",
    "NB_FILTER = 32\n",
    "NB_GRAM = 2\n",
    "    # add 1 dimension for channel\n",
    "input_shape = (1,) + arrayList[0][0].shape\n",
    "nb_char = input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4331, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
